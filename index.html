<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Three.js Example</title>
    <script async src="https://unpkg.com/es-module-shims@1.3.6/dist/es-module-shims.js"></script>

    <script type="importmap">
      {
        "imports": {
            "three": "https://unpkg.com/three@0.154.0/build/three.module.js",
            "three/addons/": "https://unpkg.com/three@0.154.0/examples/jsm/",
            "@pixiv/three-vrm": "/lib/three-vrm.module.js"
        }
      }
    </script>

    <style>
        body {
            margin: 0;
            display: flex;
            height: 100vh;
            background-image: url("img/bg.jpg");
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            display: flex;
            justify-content: center;
            align-items: center;
            align-content: space-around;
        }

        #container {
            display: flex;
            width: 100vw;
            height: 100vh;
        }

        /* Comment Box */
        #comment-box {
            width: 40%; /* Adjust the width as needed */
            height: 70%; /* Adjust the height as needed */
            background-color: rgba(0, 0, 0, 0.8); /* Dark background with transparency */
            border-radius: 10px;
            padding: 20px;
            display: flex;
            flex-direction: column;
            overflow: hidden; /* Hide scrollbars */
            font-family: 'Roboto', sans-serif;
            color: white;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3); /* Subtle shadow for depth */
            position: relative; /* Ensure positioning is relative to the container */
        }

        /* Header Inside the Comment Box */
        #comment-header {
            width: 100%;
            padding: 15px;
            font-size: 24px;
            font-family: 'Noto Sans JP', sans-serif;
            font-weight: bold;
            text-align: center;
            background: linear-gradient(45deg, #ff89a6, #ff005a); /* Gradient background */
            color: white;
            border-radius: 8px;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
            margin-bottom: 10px; /* Space between header and the decorative section */
        }

        /* Decorative Section Above the Line */
        #decorative-section {
            width: 100%;
            height: 60px; /* Adjust the height for visual impact */
            background: linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0.3)), url('path/to/decorative-pattern.png'); /* Pattern and gradient background */
            background-size: cover;
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
            margin-bottom: 10px;
        }

        /* Text in Decorative Section */
        #decorative-text {
            font-size: 18px;
            color: #ffd700; /* Gold color */
            text-shadow: 2px 2px 5px rgba(0, 0, 0, 0.5); /* Text shadow for better readability */
            font-weight: bold;
        }

        /* Extra Creative Element Section */
        #extra-creative {
            width: 100%;
            height: 80px; /* Adjust as needed */
            background: rgba(255, 255, 255, 0.2); /* Semi-transparent background */
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            margin-bottom: 10px;
            position: relative;
        }

        /* Content for the Extra Creative Section */
        #extra-creative-content {
            font-size: 20px;
            color: #ffeb3b; /* Bright yellow */
            text-align: center;
            line-height: 1.4;
            font-weight: bold;
        }

        /* Artistic Horizontal Line */
        #horizontal-line {
            width: 100%;
            height: 2px;
            background: linear-gradient(to right, transparent, #ffffff, transparent);
            margin: 20px 0; /* Space around the line */
            position: absolute;
            top: 45%; /* Position at the middle of the comment box */
            left: 0;
            transform: translateY(-50%); /* Center the line vertically */
        }

        /* Live Comments Section Below the Line */
        #comment-content {
            display: flex;
            flex-direction: column;
            gap: 10px; /* Spacing between comments */
            overflow-y: auto; /* Scrollable */
            padding-top: 60px; /* Space for the comments to be visible */
            padding-bottom: 10px; /* Space for the bottom text */
            position: absolute;
            top: 43%; /* Start below the horizontal line */
            left: 0;
            right: 0;
            bottom: 0;
            overflow: auto;
            box-sizing: border-box;
        }

        /* Each Comment Block */
        .comment {
            display: flex;
            flex-direction: column;
            padding: 8px 10px;
            background-color: rgba(255, 255, 255, 0.1); /* Semi-transparent white for comments */
            border-radius: 5px;
            animation: fade-in 0.5s ease-in-out;
            font-size: 14px;
            overflow: hidden; /* Hide overflow */
            font-family: 'Arial', sans-serif;
        }

        /* Username Styling */
        .comment .username {
            font-weight: bold;
            font-size: 15px;
            color: #ffd700; /* Gold color for the username */
        }

        /* Comment Text Styling */
        .comment .message {
            font-size: 14px;
            color: #ffffff;
            margin-top: 2px;
        }

        /* Smooth Fade-in Animation for New Comments */
        @keyframes fade-in {
            0% {
                opacity: 0;
                transform: translateY(10px);
            }
            100% {
                opacity: 1;
                transform: translateY(0);
            }
        }





        #right-side {
            width: 50%; /* Takes up more space now */
            height: 100vh; /* Full height */
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
        }
    </style>
</head>

<body>

    <div id="comment-box">
        <!-- Header Inside the Comment Box -->
        <div id="comment-header">Live Chat ‚Ä¢ Igarashi Stream</div>
    
        <!-- Decorative Section Above the Line -->
        <div id="decorative-section">
            <div id="decorative-text">Welcome to the Stream!</div>
        </div>
    
        <!-- Extra Creative Element Section -->
        <div id="extra-creative">
            <div id="extra-creative-content">
                üéâ Enjoy the Show! üéä<br>
                „Åì„Åì„ÅßÁõõ„Çä‰∏ä„Åå„Çç„ÅÜÔºÅ<br> <!-- Japanese Text for Excitement -->
                Comments from the user! üí¨
            </div>
        </div>

        <!-- Artistic Horizontal Line in the Middle -->
        <div id="horizontal-line"></div>
    
        <!-- Comment Content Below the Line -->
        <div id="comment-content"></div>
    </div>
    

    
    <div id="right-side">
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.9.1/gsap.min.js"></script>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { BVHLoader } from 'three/addons/loaders/BVHLoader.js';
        import { VRMLoaderPlugin, VRMUtils, VRM, VRMExpression, VRMExpressionMorphTargetBind  } from '@pixiv/three-vrm';
        import { FontLoader } from 'three/addons/loaders/FontLoader.js';
        import { TextGeometry } from 'three/addons/geometries/TextGeometry.js';

        let mixer = null;
        let joinSpringManager = null;
        let currentVrm = undefined;
        let animationClip;
        let mapAction = new Map();
        let currentBvhFilePath = '';

        const imageFile = new Map();
        var scene = new THREE.Scene();
        var camera = new THREE.PerspectiveCamera(45, window.innerWidth / window.innerHeight, 0.1, 10);
        camera.position.set(0, 1.5, -1.0); // Adjusted to focus on the upper body
        camera.rotation.set(0, Math.PI, 0);

        let now = new Date(new Date().toUTCString());

        const rightSide = document.getElementById('right-side');
        const renderer = new THREE.WebGLRenderer({ alpha: true });
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.setSize(rightSide.clientWidth, rightSide.clientHeight);
        rightSide.appendChild(renderer.domElement);

        window.addEventListener('resize', () => {
            renderer.setSize(rightSide.clientWidth, rightSide.clientHeight);
        });



        const targetAspect = document.body.clientWidth / document.body.clientHeight;

        const light = new THREE.DirectionalLight(0xffffff);
        light.position.set(-1, 1, -1).normalize();
        scene.add(light);

        const canvas = document.createElement('canvas');
        // canvas.width = 400; // Adjust size as needed
        // canvas.height = 512; // Adjust size as needed
        const context = canvas.getContext('2d');

        const gradient = context.createLinearGradient(0, 0, canvas.width, canvas.height);
        gradient.addColorStop(0, '#ff6f61'); // Start color (coral)
        gradient.addColorStop(1, '#ffb74d'); // End color (orange)

        const commentBox = document.getElementById('comment-content');
        const maxVisibleComments = 6; // Maximum number of visible comments
        let commentQueue = []; // Queue for comments that need audio responses
        let isProcessingAudio = false; // Flag to indicate if audio is being processed
        let introducedUsers = new Set(); // Set to keep track of introduced users
        let idleInterval; // Interval to handle idle talking

        // Function to add comments to the box
        function addCommentToBox(commentText) {
            const comment = document.createElement('div');
            comment.className = 'comment';
            comment.textContent = commentText;

            // Add the new comment at the end
            commentBox.appendChild(comment);

            // Remove the oldest comment if the total exceeds maxVisibleComments
            const comments = commentBox.querySelectorAll('.comment');
            if (comments.length > maxVisibleComments) {
                commentBox.removeChild(comments[0]); // Remove the oldest comment (first in the list)
            }
        }

        // will queue the comment and process it when the audio is done
        function handleNewComment(author, message, status) {
            // Queue the comment
            addCommentToBox(`${author}: ${message}`);
            commentQueue.push({ author, message, status });
            processAudioQueue(); // Start processing the queue if not already processing
            // playAudio();
        }

        function handleIdleMessage(message){
            commentQueue.push({ message });
            processAudioQueue(); // Start processing the queue if not already processing
            // playAudio();
        }

        const chatbotApiUrl = "http://18.176.84.155:5000/live";
        const speechApiUrl = "http://13.114.188.215:5000/voice";

        // get a response for comments from the chatbot
        async function getChatbotResponse(comment, username, id) {
            const payload = {
                body: {
                    event_id: id,
                    game_id: null,
                    character_name: "nekohime",
                    username: username,
                    userid: "Live",
                    comment: comment,
                    giftid: null,
                    gender: "male",
                    embedding: null,
                    language: "Japanese"
                }
            };

            try {
                const response = await fetch(chatbotApiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (response.ok) {
                    const responseData = await response.json();
                    console.log("response data from the chatbot:",  responseData, comment, username)
                    return {
                        text: responseData.body?.text || 'No response from chatbot',
                        emotion: responseData.body?.EmotionalB || responseData.body?.EmotionalA || "3" // Replace EmotionalB with the actual key
                    };
                } else {
                    console.error(`Chatbot request failed with status: ${response.status}`);
                    return null;
                }
            } catch (error) {
                console.error(`Failed to get chatbot response: ${error}`);
                return null;
            }
        }

        // should  off the intro audio.

        // instead have play audio function who can play this as well as other audio.

        async function playIntroAudio() {
            const introAudioUrl = 'intro.wav'; // Replace with the actual path to the intro audio file

            try {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const response = await fetch(introAudioUrl);
                const audioData = await response.arrayBuffer();
                const buffer = await audioContext.decodeAudioData(audioData);
                const source = audioContext.createBufferSource();
                source.buffer = buffer;
                source.connect(audioContext.destination);
                source.start(0);
                simulateSpeaking();
                

                console.log('Intro audio playing for new user.');

                // Ensure no overlap by waiting for the audio to finish before proceeding
                return new Promise(resolve => {
                    source.onended = () => {
                        stopMouthAnimation();
                        console.log('Intro audio finished.');
                        resolve();
                    };
                });
            } catch (error) {
                console.error('Error playing intro audio:', error);
            }
        }


        // change this to play audio function
        async function processAudioQueue() {
            if (isProcessingAudio || commentQueue.length === 0) return;

            isProcessingAudio = true;
            const { author, message, status } = commentQueue.shift(); // Get the next comment to process

            try {
                let { text: chatbotResponse, emotion: emotionText } = await getChatbotResponse(message, author, 1);

                const currentEmotion = {
                    1: 'happy',      // Very happy
                    2: 'relaxed',    // Happy
                    3: 'neutral',    // Normal
                    4: 'angry',      // Slightly confused
                    5: 'sorrow'         // Sad
                };

                applyEmotion(currentEmotion[emotionText]);
                console.log("emotion, ", currentEmotion[emotionText])
                // Now you can use chatbotResponse and emotion
                console.log(`Chatbot response: ${chatbotResponse}`);
                // console.log(`Emotion: ${emotionText}`);

                // Check if the user is new
                if (!introducedUsers.has(author)) {
                    // let introResponse = "Â§±Êïó„Åó„Å¶„ÇÇË´¶„ÇÅ„Å´„ÇÉ„ÅÑÔºÅ„Éâ„Ç∏„Å£Â≠êVTuber„ÄÅÁå´Âß´„Éç„É≠„Å†„Å´„ÇÉ„Çì„Å£ÔºÅ ";
                    // Play the intro audio for new users
                    await playIntroAudio();

                    introducedUsers.add(author); // Mark the user as introduced
                }


                if (chatbotResponse) {
                    await generateSpeech(chatbotResponse);
                }
            } catch (error) {
                console.error('Error processing the comment:', error);
            }
        }

        



        // this function will be removed
        function startIdleTalking() {
            console.log("Applying happy emotion");

            idleInterval = setInterval(async () => {
                if (commentQueue.length === 0 && !isProcessingAudio) {
                    try {
                        // Set flag to avoid overlapping messages
                        isProcessingAudio = true;

                        // Fetch idle message from the chatbot response
                        const idleComment = "";  // Identifier for idle messages
                        const username = ""; // Default username for idle mode

                        let { text: idleMessage } = await getChatbotResponse(idleComment, username, 4);

                        if (idleMessage) {
                            console.log(`Idle message from chatbot: ${idleMessage}`);
                            await generateSpeech(idleMessage);
                        } else {
                            console.error("No idle message received from the chatbot.");
                        }

                        // Reset flag after the speech finishes
                        isProcessingAudio = false;

                    } catch (error) {
                        console.error("Error fetching idle message from chatbot:", error);
                        isProcessingAudio = false; // Ensure flag is reset on error
                    }
                }
            }, 11000); // Adjust the interval as needed
        }


        // remove this function
        startIdleTalking();

        // playIntroAudio();
        let isSpeaking = false;

        // Make sure to clear the interval when the app stops, if necessary
        window.addEventListener('beforeunload', () => {
            clearInterval(idleInterval);
        });


        // Function to clear the current emotion and set it to neutral
        function clearEmotion() {
            if (!currentVrm || !currentVrm.expressionManager) {
                console.error("VRM or expression manager is not loaded.");
                return;
            }

            // Log message indicating that we're clearing the current emotion
            console.log("Clearing current emotion");

            // Reset all expressions explicitly to 0 instead of null
            Object.keys(currentVrm.expressionManager.expressions).forEach(expressionName => {
                if (currentVrm.expressionManager.getValue(expressionName) !== 0) {
                    currentVrm.expressionManager.setValue(expressionName, 0); // Set all expressions to 0
                }
            });

            // Force update to apply the reset changes
            currentVrm.expressionManager.update();

            // Verify the reset state by logging the current values of all expressions
            Object.keys(currentVrm.expressionManager.expressions).forEach(expressionName => {
                const currentValue = currentVrm.expressionManager.getValue(expressionName);
            });

            // Optionally, set the VRM model back to a neutral state
            currentVrm.expressionManager.setValue("sad", 0);
            currentVrm.expressionManager.setValue("happy", 0);
            currentVrm.expressionManager.setValue("relaxed", 0);
            currentVrm.expressionManager.setValue("sad", 0);
            currentVrm.expressionManager.setValue("angry", 0);
            currentVrm.expressionManager.setValue('neutral', 1); // 'neutral' can be changed to a suitable expression if available.
            currentVrm.expressionManager.update();

            console.log("Emotion cleared and set to neutral state.");
        }



        // I should adjust. 

        async function generateSpeech(text) {
            const params = {
                text: text,
                model_id: 12,
                speaker_name: "A-amazinGood",
                speaker_id: 0,
                sdp_ratio: 0.2,
                noise: 0.6,
                noisew: 0.8,
                length: 1,
                language: "JP",
                auto_split: "true",
                split_interval: 0.5,
                assist_text_weight: 1,
                style: "Neutral",
                style_weight: 1,
                pitch_scale: 1,
                intonation_scale: 1
            };

            const queryString = new URLSearchParams(params).toString();
            const requestUrl = `${speechApiUrl}?${queryString}`;

            try {
                const response = await fetch(requestUrl, { method: 'POST' });

                if (response.ok) {
                    const audioData = await response.arrayBuffer();
                    console.log("Speech generated successfully. Playing audio...");

                    applyTalkingEmotion();  // Apply an initial talking emotion

                    // Start switching emotions periodically during speech
                    switchEmotionsDuringSpeech();

                    // Convert the audio data to a playable format
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const buffer = await audioContext.decodeAudioData(audioData);
                    const source = audioContext.createBufferSource();
                    source.buffer = buffer;
                    source.connect(audioContext.destination);
                    source.start(0);

                    // Start mouth animation
                    simulateSpeaking();

                    // Stop mouth animation and emotion switching when audio ends
                    source.onended = () => {
                        stopMouthAnimation();
                        console.log('Audio finished playing.');
                        isProcessingAudio = false;
                        processAudioQueue(); // Continue processing the queue

                        // Stop switching emotions
                        stopSwitchingEmotions();

                        // Reset to idle/waiting emotion
                        clearEmotion();
                        switchEmotionsDuringWaiting(); // Start switching emotions during waiting
                    };

                } else if (response.status === 405) {
                    console.log("Method not allowed, retrying in 60 seconds...");
                    setTimeout(() => generateSpeech(text), 60000); // Retry after 60 seconds
                } else {
                    console.error("An unexpected error occurred:", response.statusText);
                }

            } catch (error) {
                console.error('Error processing audio:', error);
                isProcessingAudio = false;
                processAudioQueue(); // Ensure queue processing continues even if an error occurs
            }
        }


        const listener = new THREE.AudioListener();
        camera.add(listener);

        const sound = new THREE.Audio(listener);
        const audioLoader = new THREE.AudioLoader();

        // Function to load and play the audio
        async function loadAndPlayAudio(audioData) {
            const buffer = await audioContext.decodeAudioData(audioData);
            sound.setBuffer(buffer);
            sound.setLoop(false);
            sound.setVolume(0.5);
            sound.play();
            
            // When the audio finishes, process the next comment
            sound.onEnded = function() {
                console.log('Audio finished playing.');
                setTimeout(processAudioQueue, 2000); // Add a small delay before processing the next audio
            };
        }

        // Initialize EventSource for real-time comments
        function initializeCommentStream() {
            const eventSource = new EventSource('http://localhost:8000/stream-comments-only');

            eventSource.onmessage = function(event) {
                try {
                    console.log('Raw data received:', event.data); // Log the raw data
                    // Replace single quotes with double quotes to handle JSON format
                    let formattedData = event.data
                        .replace(/(\r\n|\n|\r)/gm, "")  // Remove any line breaks
                        .replace(/'/g, '"'); 

                    const commentData = JSON.parse(formattedData);

                    // Extract author and message
                    const author = commentData.author || 'Anonymous';
                    const  message = commentData.message || '';
                    const autherId = commentData.authorId || '';
                    const status = commentData.status || '';

                
                    // Handle the new comment

                    handleNewComment(author, message, status);
                    
                } catch (error) {
                    console.error('Error processing comment data:', error);
                }
            };

            eventSource.onerror = function(event) {
                console.error('Error occurred:', event);
                eventSource.close();
            };
        }

        // Start the comment stream
        initializeCommentStream();



        // Arrays to store multiple animations for each emotion
        const talkingAnimations = ['speak-emotion/Sena_Saotome_talk01_e16.bvh', 'speak-emotion/Sena_Saotome_talk02_e05.bvh', 
                                    'speak-emotion/Sena_Saotome_talk03_e03.bvh', 'speak-emotion/Sena_Saotome_talk04_e03.bvh', 
                                    'speak-emotion/Sena_Saotome_talk05_e01.bvh'];
        const waitingAnimations = ['waiting-emotion/Sena_Saotome_idle01_e06.bvh', 'waiting-emotion/Sena_Saotome_idle02_e08.bvh', 
                                    'cute-emotion/Sena_Saotome_Accent01_e11.bvh'];
        // const cuteAnimations = ['cute-emotion/Sena_Saotome_Accent01_e11.bvh', 'cute-emotion/Sena_Saotome_Approach01_e01.bvh'];

        let emotionSwitchInterval;

        // Function to get a random animation (for both talking and waiting emotions)
        function getRandomAnimation(animations) {
            const randomIndex = Math.floor(Math.random() * animations.length);
            return animations[randomIndex];
        }

        // Function to apply talking emotion
        function applyTalkingEmotion() {
            if (currentVrm) {
                const randomTalkingAnimation = getRandomAnimation(talkingAnimations);
                applyAnimation(randomTalkingAnimation, 'talking');
                console.log(`Applied talking emotion: ${randomTalkingAnimation}`);
            }
        }

        // Function to apply waiting emotion
        function applyWaitingEmotion() {
            if (currentVrm) {
                const randomWaitingAnimation = getRandomAnimation(waitingAnimations);
                applyAnimation(randomWaitingAnimation, 'waiting');
                console.log(`Applied waiting emotion: ${randomWaitingAnimation}`);
            }
        }

        // Function to switch between emotions dynamically during speech
        function switchEmotionsDuringSpeech() {
            emotionSwitchInterval = setInterval(() => {
                applyTalkingEmotion();  // Randomly switch to a different talking emotion
            }, 3000); // Switch emotions every 3 seconds (adjust this interval as needed)
        }

        // Function to switch between emotions dynamically during waiting
        function switchEmotionsDuringWaiting() {
            emotionSwitchInterval = setInterval(() => {
                applyWaitingEmotion();  // Randomly switch to a different waiting emotion
            }, 3000); // Switch emotions every 5 seconds (adjust this interval as needed)
        }

        // Call this to start switching emotions during waiting/idle phase
        function startIdleWaiting() {
            switchEmotionsDuringWaiting();  // Start the waiting emotion switching
        }


        loadVrm();
        function loadVrm() {

            const loader = new GLTFLoader();

            loader.register((parser) => {
                return new VRMLoaderPlugin(parser);
            });

            loader.load(
                `models/L001_cos01.vrm`,
                (gltf) => {
                    const vrm = gltf.userData.vrm;
                    VRMUtils.removeUnnecessaryVertices(gltf.scene);
                    VRMUtils.removeUnnecessaryJoints(gltf.scene);
                    vrm.scene.traverse((obj) => {
                        obj.frustumCulled = false;
                    });
                    scene.add(vrm.scene);
                    currentVrm = vrm;

                    vrm.scene.position.set(0.015, -1.85, 0.2); // Keep the same position
                    vrm.scene.rotation.y = 0;
                    vrm.scene.rotation.x = Math.PI / 36;
                    vrm.scene.scale.set(4.5, 2.3, 4.5); // Further increase the X and Z axes scale to widen the face more






                    joinSpringManager = vrm.springBoneManager;

                    if (currentVrm) {
                        applyWaitingEmotion();
                        
                    }
                      // Adjust the delay if needed
                    // setTimeout(() => {
                    //     console.log('success');
                    // }, 3000);
                },
                (progress) => { },
                (error) => console.error(error)
            );
        }
        

        // Function to simulate mouth movement when speaking
        function simulateSpeaking() {
            if (!currentVrm || !currentVrm.expressionManager) {
                console.error("VRM or expression manager is not loaded.");
                return;
            }

            

            const vowelExpressions = ['aa', 'ih', 'ee', 'ou', 'oh'];
            let currentExpressionIndex = 0;
            let intensity = 0;
            let intensityDirection = 1;

            const updateExpression = () => {
                if (!isSpeaking) return; // Stop updating if speaking has stopped

                // Reset all expressions
                Object.keys(currentVrm.expressionManager.expressions).forEach(expressionName => {
                    currentVrm.expressionManager.setValue(expressionName, 0);
                });

                // Update the intensity of the current expression
                const currentExpression = vowelExpressions[currentExpressionIndex];
                intensity += 0.15 * intensityDirection;

                // Check intensity bounds and adjust direction if necessary
                if (intensity >= 1 || intensity <= 0) {
                    intensityDirection *= -1;
                    intensity = Math.max(0, Math.min(1, intensity)); // Clamp intensity between 0 and 1
                }

                // Set the current expression
                currentVrm.expressionManager.setValue(currentExpression, intensity);

                // Change expression when intensity closes
                if (intensity <= 0) {
                    currentExpressionIndex = (currentExpressionIndex + 1) % vowelExpressions.length;
                }

                // Continue the loop
                setTimeout(updateExpression, 10); // Faster timing for more natural movement
            };

            isSpeaking = true; // Set speaking flag to true
            updateExpression();
        }

        function stopMouthAnimation() {
            isSpeaking = false;

            // Ensure all mouth-related expressions are reset to 0
            if (currentVrm && currentVrm.expressionManager) {
                const mouthExpressions = ['aa', 'ih', 'ee', 'ou', 'oh'];
                mouthExpressions.forEach(expressionName => {
                    currentVrm.expressionManager.setValue(expressionName, 0);
                });
                // After stopping, return to neutral emotion
            }
        }

        

        const emotions = {
            happy: ['happy'],       // Single expression
            relaxed: ['relaxed'],   // Single expression
            neutral: ['neutral'],   // Single expression
            angry: ['angry'],       // Single expression
            sad: ['sad']            // Single expression
        };

        

        

        function applyEmotion(emotion) {
            if (!currentVrm || !currentVrm.expressionManager) {
                console.error("VRM or expression manager is not loaded.");
                return;
            }

            // Reset all expressions first
            Object.keys(currentVrm.expressionManager.expressions).forEach(expressionName => {
                currentVrm.expressionManager.setValue(expressionName, 0);
            });

            
            // Apply other emotions without mouth adjustment
            if (emotions[emotion]) {
                emotions[emotion].forEach(expressionName => {
                    currentVrm.expressionManager.setValue(expressionName, 1);
                });
                console.log(`Applying emotion: ${emotion}`);
            } else {
                console.error(`Emotion ${emotion} not found.`);
            }
            
        }




        let isProcessed = true;
        let lastTimeUpdateMotion = 0;

        let isFading = false;
        let loading = false;
        let currentAction;
        let pendingAnimation = null;

        function applyAnimation(bvhFilePath, animationName ) {      
            if (!currentVrm) return; // VRM„Åå„É≠„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„Å™„Åë„Çå„Å∞‰Ωï„ÇÇ„Åó„Å™„ÅÑ
            currentVrm.lastTimeUpdateMotion = 0;
            currentVrm.loading = false;
            currentVrm.isFading = false;
            currentVrm.currentAction = null;
            currentVrm.pendingAnimation = null;
            if (currentVrm.currentBvhFilePath === bvhFilePath) return;
            if (mapAction.get(bvhFilePath)) {
                if (!mixer) {
                    mixer = new THREE.AnimationMixer(currentVrm.scene);
                    currentVrm.mixer = mixer;
                }
                if (currentVrm.isFading || currentVrm.loading) {

                    currentVrm.pendingAnimation = bvhFilePath;
                    return;
                }
                synchronizeCrossFade(bvhFilePath, 1, currentVrm);
            } else {
                currentVrm.loading = true;
                const loader1 = new BVHLoader();
                loader1.load(bvhFilePath, function (result) {
                    for (let i = result.clip.tracks.length; i--;) {
                        let track = result.clip.tracks[i];
                        var value = [];
                        var extension = track.name.split('.').pop();
                        let map = new Map(Object.entries(currentVrm.humanoid.humanBones));
                        var exists = Array.from(map.keys()).find(obj => { 
                            return track.name.toLowerCase().includes(obj.toLowerCase())});
                        if(exists){
                            if(extension){
                                track.name = `${map.get(exists).node.name}.${extension}`
                            } else {
                                track.name = `${map.get(exists).node.name}`
                            }
                        };
                        if (track.name.toLowerCase().match(/.+\.([^?]+)(\?|$)/)[1] === "position") {
                            if ([].includes(track.name)) {
                                continue;
                            } else {
                                result.clip.tracks.splice(i, 1);
                            }
                        } else {
                            if ([].includes(track.name)) {
                                result.clip.tracks.splice(i, 1);
                            } else {
                                for (var trackValue of track.values) {
                                    var trackValueNew = trackValue;
                                    if (track.values.indexOf(trackValue) % 4 == 3) {
                                        trackValueNew = -trackValue;
                                    }
                                    if (track.values.indexOf(trackValue) % 4 == 1) {
                                        trackValueNew = -trackValue;
                                    }
                                    value.push(trackValueNew);
                                }
                            }
                            track.values = value;
                            track.values.splice(0, 4);
                            track.times = track.times.subarray(1);
                        }
                    }
                    if (!mixer) {
                        mixer = new THREE.AnimationMixer(currentVrm.scene);
                        currentVrm.mixer = mixer;
                    }
                    currentVrm.animationClip = result.clip;
                    currentVrm.animationClip.name = bvhFilePath;
                    currentVrm.animationClip.duration = calculateDuration(result.clip);
                    console.log('duration', currentVrm.animationClip.duration);

                    currentVrm.loading = false;
                    if (currentVrm.isFading || currentVrm.loading) {

                        currentVrm.pendingAnimation = bvhFilePath;
                        return;
                    }
                    mapAction.set(bvhFilePath, currentVrm.animationClip);

                    synchronizeCrossFade(bvhFilePath, 1, currentVrm);
                });
            }
            currentVrm.currentBvhFilePath = bvhFilePath;
        }

        function getIndex(nameKey){
            var index = -1;
            currentVrm.scene.traverse((obj) => {
                if(index >= 0) return;
                if(obj.isMesh && obj.name.toLowerCase().includes('face')){
                    console.log(obj.geometry.userData['targetNames']);
                    if(obj.geometry.userData['targetNames']){
                        var thisIndex = obj.geometry.userData['targetNames'].indexOf(nameKey)
                        console.log(thisIndex);
                        index = thisIndex;
                    }
                }
            });
            return index;
        }

        function synchronizeCrossFade(bvhFilePath, duration) {
            var currentDate = new Date().getTime();
            if ((currentDate - lastTimeUpdateMotion) < 1000) {
                return;
            }
            lastTimeUpdateMotion = currentDate;
            console.log(new Date().toISOString());
            const loadAndApply = (clip) => {
                if (currentAction) {
                    currentAction.fadeOut(duration);
                    isFading = true;
                    console.log(duration * 1000);
                    setTimeout(() => {
                        console.log('pendingAnimation');
                        isFading = false;
                        if (pendingAnimation) {
                            applyAnimation(
                                pendingAnimation,
                                pendingAnimation.animationName
                            );
                            pendingAnimation = null;
                        }
                    }, duration * 1000);
                }

                const newAction = mixer.clipAction(clip);
                newAction.setLoop(THREE.LoopRepeat);
                newAction.clampWhenFinished = false;
                console.log('playEndAction', new Date());
                newAction.reset();
                newAction.fadeIn(duration);
                newAction.play();

                currentAction = newAction;
            };

            if (mapAction.has(bvhFilePath)) {
                loadAndApply(mapAction.get(bvhFilePath));
            }
        }

        function isSameType(pendingAnimation) {
            var pendingNotNumber = pendingAnimation.replace(/[0-9]/g, '').replace('.bvh', '');
            var currentNotNumber = currentBvhFilePath.replace(/[0-9]/g, '').replace('.bvh', '');
            console.log(pendingNotNumber === currentNotNumber);
            return pendingNotNumber === currentNotNumber;
        }

        function calculateDuration(bvhClip) {
            const frames = bvhClip.tracks[0].times.length;
            const frameTime = bvhClip.tracks[0].times[1] - bvhClip.tracks[0].times[0];
            return frames * frameTime;
        }

        animate();
        function animate() {

            setTimeout(() => {
                animate();
            }, 0.02 * 1000)

            renderer.render(scene, camera);

            if (mixer) mixer.update(0.02 );

            if (joinSpringManager) joinSpringManager.update(10 / 1000);

            if (currentVrm && currentVrm.expressionManager) {
                currentVrm.expressionManager.update();
            }

        }
    
    </script>
    
</body>

</html>